y_test <- y[-train_indices]
# Convert data into DMatrix format for XGBoost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)
# Set XGBoost parameters
params <- list(
objective = "reg:squarederror",  # For regression
booster = "gbtree",              # Use tree-based boosting
eta = 0.1,                       # Learning rate
max_depth = 6,                   # Maximum depth of a tree
subsample = 0.8,                 # Fraction of data to be used for each tree
colsample_bytree = 0.8           # Fraction of features to be used for each tree
)
# Train the model
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 100,           # Number of boosting rounds (trees)
watchlist = list(train = dtrain, test = dtest),  # To evaluate on both training and test sets
early_stopping_rounds = 10,  # Stop early if no improvement
print_every_n = 10  # Print progress every 10 rounds
)
# Make predictions on the test set
y_pred <- predict(xgb_model, newdata = dtest)
# View the first few predictions
head(y_pred)
# Calculate RMSE
rmse <- sqrt(mean((y_pred - y_test)^2))
print(paste("RMSE:", rmse))
# Create new data for prediction (e.g., with Mileage = 15000)
new_data <- kbb
new_data$Mileage <- 15000
# Convert new_data into a sparse matrix for prediction
X_new <- sparse.model.matrix(Price ~ Mileage + Make + Model + Trim + Type + Cylinder + Liter + Doors + Cruise + Sound + Leather - 1, data = new_data)
# Predict resale price at 15,000 miles
y_pred_15000 <- predict(xgb_model, newdata = X_new)
# Add predictions to new_data
new_data$PredictedPrice <- y_pred_15000
# View the top 10 cars with the highest resale values at 15,000 miles
head(new_data[order(-new_data$PredictedPrice), ], 10)
# view just the highest resale value car at 15,000 miles
best_car <- new_data[which.max(new_data$PredictedPrice), ]
print(best_car)
predict(kbb.lm,
newdata = data.frame(Make = "Cadillac",
Model = "CTS",
Doors = 4,
Trim = "Sedan 4D",
Type = "Sedan",
Mileage = 17000,
Cylinder = 6,
Liter = 2.8,
Cruise = 1,
Sound = 1,
Leather = 1)
)
View(kbb)
kbb %>% select(-residuals, -fitted)
kbb <- kbb %>% select(-residuals, -fitted)
library(tidyverse)
library(ggfortify)
library(vroom)
library(car)
library(corrplot)
library(patchwork)
library(GGally)
library(xgboost)
library(Matrix)
library(MASS)
# read in dataset
kbb <- vroom("KBB.csv")
# Clean Data
# kbb$Trim <- gsub("Sedan|4D|Conv|2D|Hatchback|Coupe|Hback|wagon|Wagon", "", kbb$Trim)
# kbb$Trim <- trimws(kbb$Trim)
### EDA
# correlation matrix for numeric variables
corrplot::corrplot(cor(kbb[c(1,2,7,8,9,10,11,12)]))
# make boxplots for categorical variables
boxplot(Price ~ Make, data = kbb)
boxplot(Price ~ Model, data = kbb)
boxplot(Price ~ Trim, data = kbb)
boxplot(Price ~ Type, data = kbb)
ggplot(kbb, aes(x = Model, y = Price, fill = Make)) +
geom_boxplot() +
labs(title = "Boxplot of Price by Model and Make", x = "Model", y = "Price") +
theme_minimal() +
scale_fill_manual(values = rainbow(length(unique(kbb$Make))))  #
# fit a linear model
kbb.lm <- lm(Price ~ Mileage + Make + Type + Liter + Cylinder + Cruise + Sound + Leather + Mileage*Make, data = kbb)
print(summary(kbb.lm))
# trying some other stuff
test1 <- lm(Price ~ Mileage + Make + Type + Liter + Cruise + Sound + Leather + Mileage*Make, data = kbb)
summary(test1)
library(boot)
kbb.glm <- glm(Price ~ Mileage + Make + Type + Liter + Cylinder + Cruise + Sound + Leather + Mileage*Make, data = kbb, family = gaussian)
cv_result <- cv.glm(kbb, kbb.glm, K = 10)  # K is the number of folds, e.g., 10-fold CV
sqrt(cv_result$delta[1])
kbb.glm <- glm(Price ~ Mileage + Make + Liter + Trim + Model + Cruise + Sound + Leather + Mileage*Make, data = kbb, family = gaussian)
cv_result <- cv.glm(kbb, kbb.glm, K = 10)  # K is the number of folds, e.g., 10-fold CV
kbb <- kbb %>% select(-residuals, -fitted)
kbb <- kbb %>% dplyr::select(-residuals, -fitted)
View(kbb)
ggplot(kbb, aes(x = Mileage, y = Price, color = Make)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
```{r, echo = FALSE, warning = FALSE}
ggplot(kbb, aes(x = Mileage, y = Price, color = Make)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
```{r, echo = FALSE, message = FALSE}
ggplot(kbb, aes(x = Mileage, y = Price, color = Make)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
Makes <- coef(test)[grepl("^Make(S|C|P|B)", names(coef(test)))]
Makes <- c(Makes, c(Buick = 0))
Mileage <- coef(test)[grepl("Mileage$", names(coef(test)))]
Mileage <- c(Mileage, c(Buick = 0))
Make_Mileage <- coef(test)[grepl("Mileage:", names(coef(test)))]
Make_Mileage <- c(Make_Mileage, Mileage:Buick = 0)
Make_Mileage <- c(Make_Mileage, `Mileage:Buick` = 0)
Makes
Makes <- c(Makes, c(MakeBuick = 0))
Mileage <- coef(test)[grepl("Mileage$", names(coef(test)))]
Mileage <- c(Mileage, c(MileageBuick = 0))
Make_Mileage <- coef(test)[grepl("Mileage:", names(coef(test)))]
Make_Mileage <- c(Make_Mileage, `Mileage:Buick` = 0)
Makes <- coef(test)[grepl("^Make(S|C|P|B)", names(coef(test)))]
Makes <- c(Makes, c(MakeBuick = 0))
Mileage <- coef(test)[grepl("Mileage$", names(coef(test)))]
Mileage <- c(Mileage, c(MileageBuick = 0))
Make_Mileage <- coef(test)[grepl("Mileage:", names(coef(test)))]
Make_Mileage <- c(Make_Mileage, `Mileage:Buick` = 0)
Makes
Mileage
MakeMileage
Make_Mileage
# Models <- coef(test)[grepl("Model", names(coef(test)))]
# Trims <- coef(test)[grepl("Trim", names(coef(test)))]
Types <- coef(test)[grepl("Type", names(coef(test)))]
Cylinder <- coef(test)[grepl("Cylinder", names(coef(test)))]
Liter <- coef(test)[grepl("Liter", names(coef(test)))]
# Door <- coef(test)[grepl("Door", names(coef(test)))]
Cruise <- coef(test)[grepl("Cruise", names(coef(test)))]
Sound <- coef(test)[grepl("Sound", names(coef(test)))]
Leather <- coef(test)[grepl("Leather", names(coef(test)))]
######### THIS IS NOT ACCURATE YET, ADJUST FOR BASE CASES!!#####
Make_15000 <- Make_Mileage*15000 + Makes
paste("At 15000 Miles, the highest make resale value is a", names(Make_15000)[which.max(Make_15000)], Make_15000[which.max(Make_15000)])
paste("Highest Type:", names(Types)[which.max(Types)], Makes[which.max(Types)])
View(kbb)
Types <- c(Types, Convertible = 0)
paste("Highest Type:", names(Types)[which.max(Types)], Makes[which.max(Types)])
Types
Types <- c(Types, TypeConvertible = 0)
paste("Highest Type:", names(Types)[which.max(Types)], Makes[which.max(Types)])
Types
library(tidyverse)
library(ggfortify)
library(vroom)
library(car)
library(corrplot)
library(patchwork)
library(GGally)
library(xgboost)
library(Matrix)
library(MASS)
# read in dataset
kbb <- vroom("KBB.csv")
# Clean Data
# kbb$Trim <- gsub("Sedan|4D|Conv|2D|Hatchback|Coupe|Hback|wagon|Wagon", "", kbb$Trim)
# kbb$Trim <- trimws(kbb$Trim)
### EDA
# correlation matrix for numeric variables
corrplot::corrplot(cor(kbb[c(1,2,7,8,9,10,11,12)]))
# make boxplots for categorical variables
boxplot(Price ~ Make, data = kbb)
boxplot(Price ~ Model, data = kbb)
boxplot(Price ~ Trim, data = kbb)
boxplot(Price ~ Type, data = kbb)
ggplot(kbb, aes(x = Model, y = Price, fill = Make)) +
geom_boxplot() +
labs(title = "Boxplot of Price by Model and Make", x = "Model", y = "Price") +
theme_minimal() +
scale_fill_manual(values = rainbow(length(unique(kbb$Make))))  #
ggplot(kbb, aes(x = Mileage, y = Price, color = Make)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
# fit a linear model
kbb.lm <- lm(Price ~ Mileage + Make + Type + Liter + Cylinder + Cruise + Sound + Leather + Mileage*Make, data = kbb)
print(summary(kbb.lm))
# trying some other stuff
test1 <- lm(Price ~ Mileage + Make + Type + Liter + Cruise + Sound + Leather + Mileage*Make, data = kbb)
summary(test1)
library(boot)
kbb.glm <- glm(Price ~ Mileage + Make + Type + Liter + Cylinder + Cruise + Sound + Leather + Mileage*Make, data = kbb, family = gaussian)
cv_result <- cv.glm(kbb, kbb.glm, K = 10)  # K is the number of folds, e.g., 10-fold CV
sqrt(cv_result$delta[1])
kbb.glm <- glm(Price ~ Mileage + Make + Liter + Trim + Model + Cruise + Sound + Leather + Mileage*Make, data = kbb, family = gaussian)
cv_result <- cv.glm(kbb, kbb.glm, K = 10)  # K is the number of folds, e.g., 10-fold CV
kbb <- kbb %>% dplyr::select(-residuals, -fitted)
library(tidyverse)
library(ggfortify)
library(vroom)
library(car)
library(corrplot)
library(patchwork)
library(GGally)
library(xgboost)
library(Matrix)
library(MASS)
# read in dataset
kbb <- vroom("KBB.csv")
# Clean Data
# kbb$Trim <- gsub("Sedan|4D|Conv|2D|Hatchback|Coupe|Hback|wagon|Wagon", "", kbb$Trim)
# kbb$Trim <- trimws(kbb$Trim)
### EDA
# correlation matrix for numeric variables
corrplot::corrplot(cor(kbb[c(1,2,7,8,9,10,11,12)]))
# make boxplots for categorical variables
boxplot(Price ~ Make, data = kbb)
boxplot(Price ~ Model, data = kbb)
boxplot(Price ~ Trim, data = kbb)
boxplot(Price ~ Type, data = kbb)
ggplot(kbb, aes(x = Model, y = Price, fill = Make)) +
geom_boxplot() +
labs(title = "Boxplot of Price by Model and Make", x = "Model", y = "Price") +
theme_minimal() +
scale_fill_manual(values = rainbow(length(unique(kbb$Make))))  #
ggplot(kbb, aes(x = Mileage, y = Price, color = Make)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
# fit a linear model
kbb.lm <- lm(Price ~ Mileage + Make + Type + Liter + Cylinder + Cruise + Sound + Leather + Mileage*Make, data = kbb)
print(summary(kbb.lm))
# trying some other stuff
test1 <- lm(Price ~ Mileage + Make + Type + Liter + Cruise + Sound + Leather + Mileage*Make, data = kbb)
summary(test1)
library(boot)
kbb.glm <- glm(Price ~ Mileage + Make + Type + Liter + Cylinder + Cruise + Sound + Leather + Mileage*Make, data = kbb, family = gaussian)
cv_result <- cv.glm(kbb, kbb.glm, K = 10)  # K is the number of folds, e.g., 10-fold CV
sqrt(cv_result$delta[1])
kbb.glm <- glm(Price ~ Mileage + Make + Liter + Trim + Model + Cruise + Sound + Leather + Mileage*Make, data = kbb, family = gaussian)
cv_result <- cv.glm(kbb, kbb.glm, K = 10)  # K is the number of folds, e.g., 10-fold CV
kbb <- kbb %>% dplyr::select(-residuals, -fitted)
clear
q()
tinytex::install_tinytex()
ls
ls
q()
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
setwd("~/School Projects/STAT536/STAT536HW2")
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
View(df)
View(meta)
y = df$Metric
X = df %<% select('Metric')
library(tidyverse)
X = df %<% select('Metric')
X <- df %>% select('Metric')
View(X)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
library(glmnet)
y <- df$Metric
X <- df %>% select(-Metric)
lasso.model = glmnet(x=X,y=gene$Malignant, alpha = 1)
View(X)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
library(glmnet)
y <- df %>% select(Metric)
X <- df %>% select(-Metric)
lasso.model = glmnet(x=X,y=y, alpha = 1)
y <- df$Metric
X <- df %>% select(-Metric)
lasso.model = glmnet(x=X,y=y, alpha = 1)
summary(lasso.model)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
library(glmnet)
y <- df$Metric
X <- df %>% select(-Metric)
lasso.model = glmnet(x=X,y=y, alpha = 1)
plot(lasso.model)
plot(lasso.model$lambda,lasso.model$dev.ratio)
lasso.cv = cv.glmnet(x=X,y=y, alpha = 1)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
library(glmnet)
y <- df$Metric
X <- df %>% select(-Metric)
lasso.model = glmnet(x=X,y=y, alpha = 1)
plot(lasso.model)
plot(lasso.model$lambda,lasso.model$dev.ratio)
lasso.cv = cv.glmnet(x=as.matrix(X),y=y, alpha = 1)
plot(lasso.cv)
lasso.cv$lambda.min
lasso.cv$lambda.1se
betahats = coef(lasso.cv, s = lasso.cv$lambda.min)
sum(betahats!=0)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
library(glmnet)
y <- df$Metric
X <- df %>% select(-Metric)
lasso.model = glmnet(x=X,y=y, alpha = 1)
plot(lasso.model)
plot(lasso.model$lambda,lasso.model$dev.ratio)
lasso.cv = cv.glmnet(x=as.matrix(X),y=y, alpha = 1)
plot(lasso.cv)
# Extract optimal lambda values
lambda_min <- lasso.cv$lambda.min
lambda_1se <- lasso.cv$lambda.1se
# Get the coefficients for the best lambda
betahats_min <- coef(lasso.cv, s=lambda_min)
betahats_1se <- coef(lasso.cv, s=lambda_1se)
# Print coefficients
print("Coefficients for lambda.min:")
print(betahats_min)
print("Coefficients for lambda.1se:")
print(betahats_1se)
# Summary statistics similar to lm
# Note: glmnet does not provide a summary like lm, but we can display relevant statistics
# 1. Number of non-zero coefficients
num_nonzero_min <- sum(betahats_min != 0)
num_nonzero_1se <- sum(betahats_1se != 0)
# 2. R-squared for the model using lambda.min
predictions_min <- predict(lasso.model, s=lambda_min, newx=as.matrix(X))
r_squared_min <- 1 - sum((y - predictions_min)^2) / sum((y - mean(y))^2)
# Print results
cat("Number of non-zero coefficients for lambda.min:", num_nonzero_min, "\n")
cat("Number of non-zero coefficients for lambda.1se:", num_nonzero_1se, "\n")
cat("R-squared for lambda.min:", r_squared_min, "\n")
# Set Seet
set.seed(77)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
library(glmnet)
y <- df$Metric
X <- df %>% select(-Metric)
lasso.model = glmnet(x=X,y=y, alpha = 1)
plot(lasso.model)
plot(lasso.model$lambda,lasso.model$dev.ratio)
lasso.cv = cv.glmnet(x=as.matrix(X),y=y, alpha = 1)
plot(lasso.cv)
# Extract optimal lambda values
lambda_min <- lasso.cv$lambda.min
lambda_1se <- lasso.cv$lambda.1se
# Get the coefficients for the best lambda
betahats_min <- coef(lasso.cv, s=lambda_min)
betahats_1se <- coef(lasso.cv, s=lambda_1se)
# Print coefficients
print("Coefficients for lambda.min:")
print(betahats_min)
print("Coefficients for lambda.1se:")
print(betahats_1se)
# Summary statistics similar to lm
# Note: glmnet does not provide a summary like lm, but we can display relevant statistics
# 1. Number of non-zero coefficients
num_nonzero_min <- sum(betahats_min != 0)
num_nonzero_1se <- sum(betahats_1se != 0)
# 2. R-squared for the model using lambda.min
predictions_min <- predict(lasso.model, s=lambda_min, newx=as.matrix(X))
r_squared_min <- 1 - sum((y - predictions_min)^2) / sum((y - mean(y))^2)
# Print results
cat("Number of non-zero coefficients for lambda.min:", num_nonzero_min, "\n")
cat("Number of non-zero coefficients for lambda.1se:", num_nonzero_1se, "\n")
cat("R-squared for lambda.min:", r_squared_min, "\n")
print(betahats_1se)
non_zero_betahats_1se <- betahats_1se[betahats_1se != 0]
print("Non-zero coefficients for lambda.1se:")
print(non_zero_betahats_1se)
print(betahats_1se)
# Convert to a data frame for better formatting and keep names
betahats_1se_df <- as.data.frame(as.matrix(betahats_1se))
colnames(betahats_1se_df) <- c("Coefficients")
non_zero_betahats_1se_df <- betahats_1se_df[betahats_1se_df$Coefficients != 0, , drop = FALSE]
print("Non-zero coefficients for lambda.1se:")
print(non_zero_betahats_1se_df)
# Set Seet
set.seed(77)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
library(glmnet)
y <- df$Metric
X <- df %>% select(-Metric)
lasso.model = glmnet(x=X,y=y, alpha = 1)
plot(lasso.model)
plot(lasso.model$lambda,lasso.model$dev.ratio)
lasso.cv = cv.glmnet(x=as.matrix(X),y=y, alpha = 1)
plot(lasso.cv)
# Extract optimal lambda values
lambda_min <- lasso.cv$lambda.min
lambda_1se <- lasso.cv$lambda.1se
# Get the coefficients for the best lambda
betahats_min <- coef(lasso.cv, s=lambda_min)
betahats_1se <- coef(lasso.cv, s=lambda_1se)
# Print coefficients
print("Coefficients for lambda.min:")
print(betahats_min)
print("Coefficients for lambda.1se:")
print(betahats_1se)
# Get the coefficients for lambda.1se
# betahats_1se <- coef(lasso.cv, s = lasso.cv$lambda.1se)
# Convert to a data frame for better formatting and keep names
betahats_1se_df <- as.data.frame(as.matrix(betahats_1se))
colnames(betahats_1se_df) <- c("Coefficients")
non_zero_betahats_1se_df <- betahats_1se_df[betahats_1se_df$Coefficients != 0, , drop = FALSE]
print("Non-zero coefficients for lambda.1se:")
print(non_zero_betahats_1se_df)
print(betahats_min)
print(betahats_1se)
plot(lasso.model)
plot(lasso.model$lambda,lasso.model$dev.ratio)
plot(lasso.cv)
print(non_zero_betahats_1se_df)
# Set Seet
set.seed(77)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
y <- df$Metric
X <- df %>% select(-Metric)
library(glmnet)
lasso.model = glmnet(x=X,y=y, alpha = 1)
plot(lasso.model)
lasso.cv = cv.glmnet(x=as.matrix(X),y=y, alpha = 1)
plot(lasso.cv)
# Extract optimal lambda values
lambda_min <- lasso.cv$lambda.min
lambda_1se <- lasso.cv$lambda.1se
# Get the coefficients for the best lambda
betahats_min <- coef(lasso.cv, s=lambda_min)
betahats_1se <- coef(lasso.cv, s=lambda_1se)
# Print coefficients
print("Coefficients for lambda.min:")
print(length(betahats_min))
print("Coefficients for lambda.1se:")
print(length(betahats_1se))
# Convert to a data frame for better formatting and keep names
betahats_1se_df <- as.data.frame(as.matrix(betahats_1se))
colnames(betahats_1se_df) <- c("Coefficients")
non_zero_betahats_1se_df <- betahats_1se_df[betahats_1se_df$Coefficients != 0, , drop = FALSE]
print("Non-zero coefficients for lambda.1se:")
print(non_zero_betahats_1se_df)
river_lm_lasso_selection <- lm(Metric ~ gord + MeanPrec07 + CumPrec07 + CumPrec04 + bio10 + bio15 +
bio18 + cls1 + cls2 + cls5 + cls8 + meanPercentDC_ModeratelyWell +
meanPercentDC_Poor + meanPercentDC_SomewhatExcessive + Lon, data = df)
View(river_lm_lasso_selection)
summary(river_lm_lasso_selection)
# PCA Fitting
library(pls)
model.cv = pcr(y ~ as.matrix(X), data=gene, validation="CV", scale=T)
model.cv = pcr(y ~ as.matrix(X), data=df, validation="CV", scale=T)
sd_values <- sapply(df, sd, na.rm = TRUE)
print(sd_values)
print(sorted(sd_values))
print(sorted(sd_values))
print(sort(sd_values))
df <- df %>% select(-meanPercentDC_VeryPoor, -meanPercentDC_Well)
# Lasso Selection & Fitting
y <- df$Metric
# Set Seet
set.seed(77)
library(tidyverse)
df <- vroom::vroom('Rivers.csv')
meta <- vroom::vroom('Metadata.csv')
# Some EDA
sd_values <- sapply(df, sd, na.rm = TRUE)
print(sort(sd_values))
df <- df %>% select(-meanPercentDC_VeryPoor, -meanPercentDC_Well)
# Lasso Selection & Fitting
y <- df$Metric
X <- df %>% select(-Metric)
library(glmnet)
lasso.model = glmnet(x=X,y=y, alpha = 1)
plot(lasso.model)
lasso.cv = cv.glmnet(x=as.matrix(X),y=y, alpha = 1)
plot(lasso.cv)
lambda_1se <- lasso.cv$lambda.1se
betahats_1se <- coef(lasso.cv, s=lambda_1se)
betahats_1se_df <- as.data.frame(as.matrix(betahats_1se))
colnames(betahats_1se_df) <- c("Coefficients")
non_zero_betahats_1se_df <- betahats_1se_df[betahats_1se_df$Coefficients != 0, , drop = FALSE]
print("Non-zero coefficients for lambda.1se:")
print(non_zero_betahats_1se_df)
river_lm_lasso_selection <- lm(Metric ~ gord + MeanPrec07 + CumPrec07 + CumPrec04 + bio10 + bio15 +
bio18 + cls1 + cls2 + cls5 + cls8 + meanPercentDC_ModeratelyWell +
meanPercentDC_Poor + meanPercentDC_SomewhatExcessive + Lon, data = df)
summary(river_lm_lasso_selection)
# PCA Fitting
library(pls)
model.cv = pcr(y ~ as.matrix(X), data=df, validation="CV", scale=T)
summary(model.cv)
validationplot(model.cv)
plot(model.cv, "validation", val.type = "R2", legendpos = "bottomright")
model.2 = pcr(Malignant ~ X, data=gene, ncomp=2, scale=T)
model.2 = pcr(y ~ as.matrix(X), data=df, ncomp=2, scale=T)
model.cv = pcr(y ~ as.matrix(X), data=df, validation="CV", scale=T)
summary(model.cv)
model.2 = pcr(y ~ as.matrix(X), data=df, ncomp=2, scale=T)
summary(model.2)
